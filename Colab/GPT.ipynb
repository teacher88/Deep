{"cells":[{"cell_type":"markdown","metadata":{"id":"YB25QkdO7lKM"},"source":["#1.GPT - Generative Pre-trained Transformer\n","- https://wikidocs.net/186266"]},{"cell_type":"markdown","metadata":{"id":"n-Af10az7oWL"},"source":["##1-1.설정"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2319,"status":"ok","timestamp":1747607235148,"user":{"displayName":"강사","userId":"14202961198716200052"},"user_tz":-540},"id":"8JC6dYRq4T0k","outputId":"a8f69c99-28a6-4c03-c003-53a71b0a725e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tAz0oiXR5WTq"},"outputs":[],"source":["import numpy as np\n","import random\n","from tqdm import tqdm\n","import tensorflow as tf\n","from transformers import AutoTokenizer\n","from transformers import TFGPT2LMHeadModel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5326,"status":"ok","timestamp":1747607331815,"user":{"displayName":"강사","userId":"14202961198716200052"},"user_tz":-540},"id":"2cDxxt_Q5ZTY","outputId":"744057de-b446-449e-8b80-aa3cb9d6c5c4"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.10.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'lm_head.weight', 'transformer.h.6.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.3.attn.masked_bias']\n","- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"]}],"source":["model=TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)  # 두 개의 문장이 이어지는 문장 관계인지 여부를 판단하는 GPT 구조를 로드\n","tokenizer=AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')    # 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드"]},{"cell_type":"markdown","metadata":{"id":"cYV1WXx57-th"},"source":["##1-2. KoGPT-2로 문장 생성하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WEd-0dG65dQw"},"outputs":[],"source":["sent='근육이 커지기 위해서는'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1747607389921,"user":{"displayName":"강사","userId":"14202961198716200052"},"user_tz":-540},"id":"GssUn35c5-bU","outputId":"6d408816-9fc3-486d-8c19-f6cf44680208"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([[33245 10114 12748 11357]], shape=(1, 4), dtype=int32)\n"]}],"source":["input_ids=tokenizer.encode(sent)\n","input_ids=tf.convert_to_tensor([input_ids])\n","print(input_ids)    #  1) 5개의 정수 시퀀스"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28873,"status":"ok","timestamp":1747607420000,"user":{"displayName":"강사","userId":"14202961198716200052"},"user_tz":-540},"id":"Pn_X2BcH6MYM","outputId":"4e90a0e9-7c2f-44bc-e58f-2ec216ca8f9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[33245, 10114, 12748, 11357, 23879, 39306, 9684, 7884, 10211, 15177, 26421, 387, 17339, 7889, 9908, 15768, 6903, 15386, 8146, 12923, 9228, 18651, 42600, 9564, 17764, 9033, 9199, 14441, 7335, 8704, 12557, 32030, 9510, 18595, 9025, 10571, 25741, 10599, 13229, 9508, 7965, 8425, 33102, 9122, 21240, 9801, 32106, 13579, 12442, 13235, 19430, 8022, 12972, 9566, 11178, 9554, 24873, 7198, 9391, 12486, 8711, 9346, 7071, 36736, 9693, 12006, 9038, 10279, 36122, 9960, 8405, 10826, 18988, 25998, 9292, 7671, 9465, 7489, 9277, 10137, 9677, 9248, 9912, 12834, 11488, 13417, 7407, 8428, 8137, 9430, 14222, 11356, 10061, 9885, 19265, 9377, 20305, 7991, 9178, 9648, 9133, 10021, 10138, 30315, 21833, 9362, 9301, 9685, 11584, 9447, 42129, 10124, 7532, 17932, 47123, 37544, 9355, 15632, 9124, 10536, 13530, 12204, 9184, 36152, 9673, 9788, 9029, 11764]\n"]}],"source":["# 2) 정수 시퀀스를 GPT의 입력으로 사용하여 GPT가 이어서 문장을 생성\n","output=model.generate(input_ids,\n","                      max_length=128,\n","                      repetition_penalty=2.0,  # 문장 생성 시 동일한 단어나 구가 반복되는 것을 방지 1.0 ~ 2.0+\n","                      use_cache=True)          # 계산 결과를 캐시에 저장 >> 생성 속도를 빠르게 하기 위한 옵션\n","\n","output_ids=output.numpy().tolist()[0]\n","print(output_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1747607423334,"user":{"displayName":"강사","userId":"14202961198716200052"},"user_tz":-540},"id":"XRihGr5r6Xnk","outputId":"126646a7-68cd-4630-9c6a-e3e096948e96"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'근육이 커지기 위해서는 무엇보다 규칙적인 생활습관이 중요하다.\\n특히, 아침식사는 단백질과 비타민이 풍부한 과일과 채소를 많이 섭취하는 것이 좋다.\\n또한 하루 30분 이상 충분한 수면을 취하는 것도 도움이 된다.\\n아침 식사를 거르지 않고 규칙적으로 운동을 하면 혈액순환에 도움을 줄 뿐만 아니라 신진대사를 촉진해 체내 노폐물을 배출하고 혈압을 낮춰준다.\\n운동은 하루에 10분 정도만 하는 게 좋으며 운동 후에는 반드시 스트레칭을 통해 근육량을 늘리고 유연성을 높여야 한다.\\n운동 후 바로 잠자리에 드는 것은 피해야 하며 특히 아침에 일어나면 몸이 피곤해지기 때문에 무리하게 움직이면 오히려 역효과가 날 수도 있다.\\n운동을'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}],"source":["tokenizer.decode(output_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":202,"status":"ok","timestamp":1747607424764,"user":{"displayName":"강사","userId":"14202961198716200052"},"user_tz":-540},"id":"1dLKrPre6uOi","outputId":"5b427bf8-6e6a-4f4e-8283-bb4edd927de4"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 4, 51200)\n"]}],"source":["# Numpy로 Top 5 뽑기\n","output=model(input_ids)\n","print(output.logits.shape)  # 1개 문장, 길이 4, 단어 사전 크기 30522\n","\n","top5=tf.math.top_k(output.logits[0, -1], k=5)  # [0 ~ 마지막]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1747607425783,"user":{"displayName":"강사","userId":"14202961198716200052"},"user_tz":-540},"id":"9C_IFWTJ647E","outputId":"0a240adf-f13f-4e9d-c3f9-3827d88b5379"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['▁무엇보다', '▁우선', '▁반드시', '▁피부', '▁무엇보다도']"]},"metadata":{},"execution_count":9}],"source":["tokenizer.convert_ids_to_tokens(top5.indices.numpy())    # 그 후 Top 5의 단어를 한국어로 변환하여 출력"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":10431,"status":"ok","timestamp":1747607437208,"user":{"displayName":"강사","userId":"14202961198716200052"},"user_tz":-540},"id":"zugf2Woq68uA","outputId":"c013b6e5-af9a-4686-a9cf-2ad001572daa"},"outputs":[{"output_type":"stream","name":"stdout","text":["[33245, 10114, 12748, 11357]\n"]},{"output_type":"execute_result","data":{"text/plain":["'근육이 커지기 위해서는 무엇보다 면역력을 키우는 노력이 필요하다.\\n또 평소 스트레스와 피로가 쌓여있는 직장인들을 위해서는 충분한 수분을 충분히 챙기고 수분 보충에 도움이 되어야 하며, 평소보다 수분의 함량이 적은 채소도 많이 먹는 것이 좋겠지만 수분의 함량이'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}],"source":["# Numpy Top 5로 문장 생성하기\n","sent='근육이 커지기 위해서는'\n","input_ids=tokenizer.encode(sent)\n","print(input_ids)\n","\n","while len(input_ids) < 50:\n","    output=model(np.array([input_ids]))\n","\n","    # Top 5의 단어들을 추출\n","    top5 = tf.math.top_k(output.logits[0, -1], k=5)\n","\n","    # Top 5의 단어들 중 랜덤으로 다음 단어로 선택.\n","    token_id = random.choice(top5.indices.numpy())\n","    input_ids.append(token_id)\n","\n","tokenizer.decode(input_ids)"]},{"cell_type":"markdown","metadata":{"id":"8xTdis-5-QLk"},"source":["##1-3.한국어 챗봇"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-SUGJHDO_SKY"},"outputs":[],"source":["import pandas as pd\n","from tqdm.notebook import tqdm\n","import urllib.request"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3202,"status":"ok","timestamp":1747611312535,"user":{"displayName":"강사","userId":"14202961198716200052"},"user_tz":-540},"id":"_6DcyC7L7EXe","outputId":"b6985e90-c4ad-4cfb-f35a-7a2706f169be"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.10.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'lm_head.weight', 'transformer.h.6.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.3.attn.masked_bias']\n","- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"]}],"source":["# 두 개의 문장이 이어지는 문장 관계인지 여부를 판단하는 GPT 구조를 로드\n","tokenizer=AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', pad_token='<pad>')\n","\n","# 해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드\n","model=TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1747611314933,"user":{"displayName":"강사","userId":"14202961198716200052"},"user_tz":-540},"id":"DK_jGtTS-qqP","outputId":"5206b713-963b-45da-94b5-118e596cbc56"},"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","1\n","3\n","----------\n","</s>\n","<usr>\n","<pad>\n","<sys>\n"]}],"source":["print(tokenizer.bos_token_id)\n","print(tokenizer.eos_token_id)\n","print(tokenizer.pad_token_id)\n","print('-' * 10)\n","print(tokenizer.decode(1))\n","print(tokenizer.decode(2))\n","print(tokenizer.decode(3))\n","print(tokenizer.decode(4))\n","\n","# </s> 1 : 시작,끝  / <pad> 3\n","# </s><usr> 12시 땡!<sys> 하루가 또 가네요.</s>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":443,"status":"ok","timestamp":1747611320276,"user":{"displayName":"강사","userId":"14202961198716200052"},"user_tz":-540},"id":"nqCpUpmQ-rFG","outputId":"ae6ec803-cd8a-4994-8ede-8e3da6977e3f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11823, 3)"]},"metadata":{},"execution_count":52}],"source":["# 챗봇 데이터를 로드\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n","train_data=pd.read_csv('ChatBotData.csv')\n","train_data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bg9ilQZR_MhU"},"outputs":[],"source":["# 챗봇 전처리\n","def get_chat_data():\n","  for question, answer in zip(train_data.Q.to_list(), train_data.A.to_list()):\n","    bos_token=[tokenizer.bos_token_id]\n","    eos_token=[tokenizer.eos_token_id]\n","    sent=tokenizer.encode('<usr>' + question + '<sys>' + answer)   # 질문과 답변 전처리\n","    yield bos_token + sent + eos_token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_6nYM0Gd_fSA"},"outputs":[],"source":["# batch_size=32   # GPU 메모리가 부족 16 8 4\n","batch_size=16\n","dataset=tf.data.Dataset.from_generator(get_chat_data, output_types=tf.int32)"]},{"cell_type":"code","source":["# 배치 크기만큼 데이터를 구성하되 패딩 토큰으로 패딩을 진행.\n","dataset=dataset.padded_batch(batch_size=batch_size,\n","                             padded_shapes=(None,),     # 각 시퀀스의 길이가 가변적, 가장 긴 시퀀스를 기준으로 나머지 시퀀스에 패딩을 추가\n","                             padding_values=tokenizer.pad_token_id)  # 3(패딩)"],"metadata":{"id":"DP40D8REgTqr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1747611362038,"user":{"displayName":"강사","userId":"14202961198716200052"},"user_tz":-540},"id":"HMMWKzfx_lA2","outputId":"a8b12eb1-6de3-4b80-f26a-2344b7b5e64a"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[    1     2  9349  7888   739  7318   376     4 12557  6824  9108  9028\n","   7098 25856     1     3     3     3     3     3     3]\n"," [    1     2  9020  8263  7497 10192 11615  8210  8006     4 12422  8711\n","   9535  7483 12521     1     3     3     3     3     3]\n"," [    1     2  9085  7597   395  8149 10624  7397 24224 13358  7182     4\n","  12079  8135 16899  9677  8234   389     1     3     3]\n"," [    1     2  9085  7597   395  8149  9465 10624  7397 24224 13358  7182\n","      4 12079  8135 16899  9677  8234   389     1     3]\n"," [    1     2  9943   422   418  9327  8702  7098     4  9847 16912 18328\n","   8671  7415  8263  8234   389     1     3     3     3]\n"," [    1     2  9815   410 21249 10174  6824  8210  8006     4  9427 11056\n","  11594 10137 10556  9266  8711 25856     1     3     3]\n"," [    1     2  9815   410 21249  9183  7249     4  9427 11056 11594 10137\n","  10556  9266  8711 25856     1     3     3     3     3]\n"," [    1     2  9815 37655  9622  8619 10401  9183  9328   216     4  9443\n","  29490  9846  9788  9341 25856     1     3     3     3]\n"," [    1     2  9815 37655 10135  7066 39488  9122  9050  9668 16576  9277\n","   9044     4 15148 19658  9098  7652  7801 25856     1]\n"," [    1     2  9815 37655 10135  7066  7692 11848  9042  7019 20284  7254\n","      4 15148 19658  9098  7652  7801 25856     1     3]\n"," [    1     2  9815 37655 18381  9063  7489 29615  9054 15730 29452  8030\n","      4 33254 10300 23775 25856     1     3     3     3]\n"," [    1     2 19319 48397  8711     4  9022 19858 27031  9122  8046 25856\n","      1     3     3     3     3     3     3     3     3]\n"," [    1     2 19319 46651 27481 48397  8711     4  9022 19858 27031  9122\n","   8046 25856     1     3     3     3     3     3     3]\n"," [    1     2 19319  8135  9749 10225  6866  9677  7182     4  9749  9589\n","  20540  7801 25856     1     3     3     3     3     3]\n"," [    1     2 17230 17429  9160  8098     4 10855  8135  9427 35813  9122\n","   8046 25856     1     3     3     3     3     3     3]\n"," [    1     2 47980 22227 26992  7058  7182     4 26992  8137  9376  8737\n","   8236  7801 25856     1     3     3     3     3     3]], shape=(16, 21), dtype=int32)\n"]}],"source":["# 첫번째 배치(첫 32개의 데이터 묶음)을 출력해 토큰, 1(시작,끝), 3(패딩)\n","for batch in dataset:\n","    print(batch)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1747611367029,"user":{"displayName":"강사","userId":"14202961198716200052"},"user_tz":-540},"id":"Uy_HZdIQ_nLm","outputId":"080c5546-5e72-44c6-b476-8908ef211a93"},"outputs":[{"output_type":"stream","name":"stdout","text":["</s><usr> 12시 땡!<sys> 하루가 또 가네요.</s><pad><pad><pad><pad><pad><pad>\n","batch shape: (16, 21)\n","batch dtype: <dtype: 'int32'>\n"]}],"source":["print(tokenizer.decode(batch[0]))\n","print(\"batch shape:\", batch.shape)\n","print(\"batch dtype:\", batch.dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":68,"referenced_widgets":["4939e0b9737f4b08a22fbe2757316211","d6734a60eba94870b90ec7e6d4425b2b","6cf4eb10271f447089c20425f7318e10","b09d175156644db3a2b1a2acaa2c6fc5","4a8695f4b996410eb709de8c0ac33d8e","a843d23ae63a49509f6194eb2f2b379a","e97595c8d63847eeb92639a331bffb58","c6a8ae0713e24600bec569356dc0bb73","aac2e8d20b4f4f8b89cd9ac32a2839db","b02cbf3b32a049049765f83d5995844f","5ba97716d7be4e47acd6b3310ad60b78"]},"id":"jppl4t8LAENf","outputId":"fc336218-1ac6-43f3-e88e-0f30964d58eb","executionInfo":{"status":"ok","timestamp":1747611927457,"user_tz":-540,"elapsed":551795,"user":{"displayName":"강사","userId":"14202961198716200052"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/739 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4939e0b9737f4b08a22fbe2757316211"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[Epoch:    1] cost = 2.23705411\n"]}],"source":["# 옵티마이저 결정\n","adam=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n","\n","# 전체 데이터의 개수를 배치 크기로 나누면 하나의 에포크에서 실행되는 학습 횟수가 계산됨.\n","steps=len(train_data) // batch_size + 1\n","\n","EPOCHS=1   # 3, batch=32\n","\n","for epoch in range(EPOCHS):\n","  epoch_loss=0   # 한 epoch 동안의 누적 손실\n","\n","  for batch in tqdm(dataset, total=steps):   # 배치 단위로 데이터셋 순회\n","      with tf.GradientTape() as tape:        # 자동 미분 준비\n","          result=model(batch, labels=batch)  # 모델 예측 (입력과 정답이 같음: autoencoder 또는 language model)\n","          loss=result[0]   # 손실 값 추출\n","          batch_loss=tf.reduce_mean(loss)  # 손실 평균값 계산\n","\n","      grads = tape.gradient(batch_loss, model.trainable_variables)   # 손실에 대한 가중치의 gradient 계산\n","      adam.apply_gradients(zip(grads, model.trainable_variables))    # 옵티마이저로 가중치 업데이트\n","      epoch_loss += batch_loss / steps  # 평균 손실 누적\n","\n","  print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, epoch_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HS22fb0pB-86","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747611938935,"user_tz":-540,"elapsed":56,"user":{"displayName":"강사","userId":"14202961198716200052"}},"outputId":"cd663b34-0786-470b-8404-ad1b2affd37c"},"outputs":[{"output_type":"stream","name":"stdout","text":["정수 인코딩 후 : tf.Tensor([[    1     2 10070  7235 10586 12557   376     4]], shape=(1, 8), dtype=int32)\n","정수 인코딩을 재복원 : </s><usr> 오늘도 좋은 하루!<sys>\n"]}],"source":["# 챗봇 실행\n","text='오늘도 좋은 하루!'\n","sent='<usr>' + text + '<sys>'\n","\n","input_ids=[tokenizer.bos_token_id] + tokenizer.encode(sent)\n","input_ids=tf.convert_to_tensor([input_ids])\n","print('정수 인코딩 후 :', input_ids)\n","print('정수 인코딩을 재복원 :', tokenizer.decode(input_ids[0]))"]},{"cell_type":"code","source":["output=model.generate(input_ids, max_length=50, early_stopping=True, eos_token_id=tokenizer.eos_token_id)\n","decoded_sentence=tokenizer.decode(output[0].numpy().tolist())\n","print(decoded_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PClBXllGe2Lb","executionInfo":{"status":"ok","timestamp":1747611942202,"user_tz":-540,"elapsed":1652,"user":{"displayName":"강사","userId":"14202961198716200052"}},"outputId":"155eb273-b933-4260-8a94-0861ff8952ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["</s><usr> 오늘도 좋은 하루!<sys> 좋은 하루를 만들어보세요.</s>\n"]}]},{"cell_type":"code","source":["output=model.generate(input_ids, max_length=50, do_sample=True, top_k=10)\n","decoded_sentence = tokenizer.decode(output[0].numpy().tolist())\n","print(decoded_sentence.split('<sys> ')[1].replace('</s>', ''))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rQS0-xzKe7cI","executionInfo":{"status":"ok","timestamp":1747611946955,"user_tz":-540,"elapsed":2186,"user":{"displayName":"강사","userId":"14202961198716200052"}},"outputId":"6d912fae-9273-448d-a25d-45476d6b34a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["좋은 하루를 만들 수 있죠.\n"]}]},{"cell_type":"code","source":["def return_answer_by_chatbot(user_text):\n","  sent='<usr>' + user_text + '<sys>'\n","  input_ids=[tokenizer.bos_token_id] + tokenizer.encode(sent)\n","  input_ids=tf.convert_to_tensor([input_ids])\n","  output=model.generate(input_ids, max_length=50, do_sample=True, top_k=20)\n","  sentence=tokenizer.decode(output[0].numpy().tolist())\n","  chatbot_response = sentence.split('<sys> ')[1].replace('</s>', '')\n","  return chatbot_response"],"metadata":{"id":"O2r0CF94fCS3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["return_answer_by_chatbot('안녕! 반가워~')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Vu-hTjn6nkwT","executionInfo":{"status":"ok","timestamp":1747611951662,"user_tz":-540,"elapsed":629,"user":{"displayName":"강사","userId":"14202961198716200052"}},"outputId":"90e25b45-1f57-424c-dbf3-dda6d603ec51"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'감사합니다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["return_answer_by_chatbot('너는 누구야?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"LaMz1scNfNjJ","executionInfo":{"status":"ok","timestamp":1747611955766,"user_tz":-540,"elapsed":2274,"user":{"displayName":"강사","userId":"14202961198716200052"}},"outputId":"7fcf3319-55b1-4adb-b3f6-704208c4c01e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'제가 먼저 연락하고 저한테 물어보세요.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["return_answer_by_chatbot('영화 해리포터 재밌어?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"5Mysgz4vfOPe","executionInfo":{"status":"ok","timestamp":1747611960337,"user_tz":-540,"elapsed":2702,"user":{"displayName":"강사","userId":"14202961198716200052"}},"outputId":"d6281121-387c-4154-ae7b-99da05cd667f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'영화 한편 보는게 좋을 것 같아요.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":["return_answer_by_chatbot('너 딥 러닝 잘해?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"fLqVA-bnfPs2","executionInfo":{"status":"ok","timestamp":1747611964519,"user_tz":-540,"elapsed":1249,"user":{"displayName":"강사","userId":"14202961198716200052"}},"outputId":"c111ffb4-60ff-4f3e-ee9d-8924df7f8044"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'직접 해보세요.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":[],"metadata":{"id":"LJsL0uyM-tUg"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"toc_visible":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPX3WqLfKYPWjbWXPmcBMUH"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4939e0b9737f4b08a22fbe2757316211":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6734a60eba94870b90ec7e6d4425b2b","IPY_MODEL_6cf4eb10271f447089c20425f7318e10","IPY_MODEL_b09d175156644db3a2b1a2acaa2c6fc5"],"layout":"IPY_MODEL_4a8695f4b996410eb709de8c0ac33d8e"}},"d6734a60eba94870b90ec7e6d4425b2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a843d23ae63a49509f6194eb2f2b379a","placeholder":"​","style":"IPY_MODEL_e97595c8d63847eeb92639a331bffb58","value":"100%"}},"6cf4eb10271f447089c20425f7318e10":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6a8ae0713e24600bec569356dc0bb73","max":739,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aac2e8d20b4f4f8b89cd9ac32a2839db","value":739}},"b09d175156644db3a2b1a2acaa2c6fc5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b02cbf3b32a049049765f83d5995844f","placeholder":"​","style":"IPY_MODEL_5ba97716d7be4e47acd6b3310ad60b78","value":" 739/739 [09:11&lt;00:00,  1.39it/s]"}},"4a8695f4b996410eb709de8c0ac33d8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a843d23ae63a49509f6194eb2f2b379a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e97595c8d63847eeb92639a331bffb58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6a8ae0713e24600bec569356dc0bb73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aac2e8d20b4f4f8b89cd9ac32a2839db":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b02cbf3b32a049049765f83d5995844f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ba97716d7be4e47acd6b3310ad60b78":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}